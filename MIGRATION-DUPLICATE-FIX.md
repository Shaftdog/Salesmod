# Migration Duplicate Import Fix

## Problem
The migration system had a race condition bug that allowed duplicate imports when users clicked the import button multiple times quickly. This resulted in:

- Multiple migration jobs for the same file
- Duplicate records in the database
- Confusion in the import history

## Root Cause
The idempotency key included a timestamp, making every request unique and defeating the duplicate prevention logic:

```typescript
// BUGGY CODE (before fix):
const timestamp = Date.now();
const idempotencyKey = `${user.id}_${mappingHash}_${fileHash}_${timestamp}`;
```

This meant that even identical imports had different keys, allowing multiple jobs to be created.

## Solution
Implemented a two-key system:

1. **Base Idempotency Key** (without timestamp) - for duplicate detection
2. **Unique Idempotency Key** (with timestamp) - for job identification

### Code Changes

#### 1. API Logic Fix (`src/app/api/migrations/run/route.ts`)
```typescript
// Generate base idempotency key WITHOUT timestamp
const baseIdempotencyKey = `${user.id}_${mappingHash}_${fileHash}`;

// Check for existing job with same base key
const { data: existingJob } = await supabase
  .from('migration_jobs')
  .select('id, status, base_idempotency_key')
  .eq('base_idempotency_key', baseIdempotencyKey)
  .maybeSingle();

// If job is still processing, return existing job
if (existingJob && (existingJob.status === 'processing' || existingJob.status === 'pending')) {
  return NextResponse.json({
    jobId: existingJob.id,
    message: 'Job already in progress',
  });
}

// Create unique key with timestamp for this specific job
const timestamp = Date.now();
const uniqueIdempotencyKey = `${baseIdempotencyKey}_${timestamp}`;
```

#### 2. Database Schema Update (`supabase/migrations/20250123000000_fix_migration_idempotency.sql`)
```sql
-- Add base_idempotency_key column
ALTER TABLE public.migration_jobs 
ADD COLUMN IF NOT EXISTS base_idempotency_key TEXT;

-- Clean up existing duplicates by marking them as cancelled
-- Keeps only the first job for each base_idempotency_key
WITH duplicates AS (...)
UPDATE public.migration_jobs
SET status = 'cancelled'
WHERE is duplicate;

-- Add unique index to prevent race conditions (excludes cancelled jobs)
CREATE UNIQUE INDEX unique_base_idempotency_key_active
ON public.migration_jobs(user_id, base_idempotency_key)
WHERE status != 'cancelled';
```

## How It Works Now

1. **User clicks import** → System generates base key (no timestamp)
2. **Check for existing job** → Look for job with same base key
3. **If job exists and is processing** → Return existing job ID
4. **If no job or job is completed** → Create new job with unique key
5. **Database constraint** → Prevents race conditions at DB level

## Benefits

- ✅ **Prevents duplicate imports** - Same file/mapping can't be imported twice simultaneously
- ✅ **Allows re-imports** - Completed jobs can be re-imported if needed
- ✅ **Race condition safe** - Database constraint prevents concurrent duplicate jobs
- ✅ **Backward compatible** - Existing jobs continue to work
- ✅ **Better UX** - Users get clear feedback about existing jobs

## Testing

To test the fix:

1. **Single Import Test**: Import a file once - should work normally
2. **Duplicate Prevention Test**: Try to import the same file twice quickly - second should be blocked
3. **Re-import Test**: After first import completes, should allow re-importing same file
4. **Database Constraint Test**: Database should prevent duplicate base keys

## Migration Status

- ✅ API logic updated
- ✅ Database migration created
- ✅ Backward compatibility maintained
- ✅ Documentation complete

The fix is now ready for deployment and will prevent the duplicate import issue you experienced.
